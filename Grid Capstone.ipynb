{
 "cells": [
  {
   "source": [
    "## Task 1 - Build Purchases Attribution Projection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pytest import fixture\n",
    "\n",
    "\n",
    "pyspark = SparkSession.builder.master(\"local[2]\").getOrCreate()\n",
    "\n",
    "click_stream = pyspark.read.csv('tests/fixtures/clickstream_sample.tsv', sep=r'\\t', header=True)\n",
    "purchases = pyspark.read.csv('tests/fixtures/purchases_sample.tsv', sep=r'\\t', header=True)\n"
   ]
  },
  {
   "source": [
    "### Task 1.1 - Implement it by utilizing default Spark SQL capabilities."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting etl/purchase_attribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile etl/purchase_attribution.py\n",
    "\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as psf \n",
    "\n",
    "def get_sessions(click_stream):\n",
    "\n",
    "    click_session_use = click_stream.where(psf.col('eventType').isin({'app_open', 'app_close'}))\n",
    "\n",
    "    # FIXME: maybe add another window to remove join\n",
    "    w0 = Window.partitionBy('userId')\n",
    "    sessions = (click_session_use.select(\n",
    "            psf.col('userId'),\n",
    "            psf.col('eventTime').alias('sessionStart').cast('timestamp'),\n",
    "            psf.lead('eventTime').over(\n",
    "                w0.orderBy(psf.col('eventTime').asc())\n",
    "            ).alias('sessionEnd').cast('timestamp'),\n",
    "            psf.get_json_object(psf.col('attributes'), '$.channel_id').alias('channelId'),\n",
    "            psf.get_json_object(psf.col('attributes'), '$.campaign_id').alias('campaignId'),\n",
    "            psf.col('eventId').alias('sessionId')\n",
    "        ).where(psf.col('eventType') == 'app_open')\n",
    "        .orderBy(psf.col('userId').asc(), psf.col('sessionStart'))\n",
    "    )\n",
    "\n",
    "    return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Appending to etl/purchase_attribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a etl/purchase_attribution.py\n",
    "\n",
    "def get_purchase_attribution(click_stream, sessions, purchases):\n",
    "\n",
    "    click_purchases = click_stream.where(\n",
    "        psf.col('eventType') == \"purchase\"\n",
    "    ).withColumnRenamed(\n",
    "        'userId', 'purchaseUserId'\n",
    "    ).withColumn(\n",
    "        'purchaseStreamId', psf.get_json_object(psf.col('attributes'), '$.purchase_id')\n",
    "    ).drop('attributes')\n",
    "\n",
    "    purchases_stream = click_purchases.join(sessions,\n",
    "        [\n",
    "            click_purchases.purchaseUserId == sessions.userId,\n",
    "            sessions.sessionStart < click_purchases.eventTime, \n",
    "            sessions.sessionEnd > click_purchases.eventTime\n",
    "        ]\n",
    "    ).select(\n",
    "        'purchaseStreamId', 'sessionId', 'campaignId', 'channelId'\n",
    "    )\n",
    "\n",
    "    purchase_attribution = purchases_stream.join(\n",
    "        purchases, purchases.purchaseId == purchases_stream.purchaseStreamId\n",
    "    ).select(\n",
    "        'purchaseId', 'purchaseTime', 'billingCost', 'isConfirmed', 'sessionId', 'campaignId', 'channelId'\n",
    "    ).orderBy(\n",
    "        'purchaseTime'\n",
    "    )\n",
    "\n",
    "    return purchase_attribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------------------+-------------------+------------+----------+---------+\n",
      "|userId|       sessionStart|         sessionEnd|   channelId|campaignId|sessionId|\n",
      "+------+-------------------+-------------------+------------+----------+---------+\n",
      "|    u1|2019-01-01 00:00:00|2019-01-01 00:02:00|  Google Ads|      cmp1|    u1_e1|\n",
      "|    u2|2019-01-01 00:00:00|2019-01-01 00:04:00|  Yandex Ads|      cmp1|    u2_e1|\n",
      "|    u2|2019-01-02 00:00:00|2019-01-02 00:04:00|  Yandex Ads|      cmp2|    u2_e6|\n",
      "|    u3|2019-01-01 00:00:00|2019-01-01 00:02:00|Facebook Ads|      cmp2|    u3_e1|\n",
      "|    u3|2019-01-01 01:11:11|2019-01-01 01:12:30|  Google Ads|      cmp1|    u3_e5|\n",
      "|    u3|2019-01-02 02:00:00|2019-01-02 02:15:40|  Yandex Ads|      cmp2|   u3_e10|\n",
      "|    u3|2019-01-02 13:00:10|2019-01-02 13:06:00|  Yandex Ads|      cmp2|   u3_e19|\n",
      "+------+-------------------+-------------------+------------+----------+---------+\n",
      "\n",
      "+----------+-------------------+-----------+-----------+---------+----------+----------+\n",
      "|purchaseId|       purchaseTime|billingCost|isConfirmed|sessionId|campaignId| channelId|\n",
      "+----------+-------------------+-----------+-----------+---------+----------+----------+\n",
      "|        p1| 2019-01-01 0:01:05|      100.5|       TRUE|    u1_e1|      cmp1|Google Ads|\n",
      "|        p2| 2019-01-01 0:03:10|        200|       TRUE|    u2_e1|      cmp1|Yandex Ads|\n",
      "|        p3| 2019-01-01 1:12:15|        300|      FALSE|    u3_e5|      cmp1|Google Ads|\n",
      "|        p4| 2019-01-01 2:13:05|       50.2|       TRUE|   u3_e10|      cmp2|Yandex Ads|\n",
      "|        p5| 2019-01-01 2:15:05|         75|       TRUE|   u3_e10|      cmp2|Yandex Ads|\n",
      "|        p6|2019-01-02 13:03:00|         99|      FALSE|   u3_e19|      cmp2|Yandex Ads|\n",
      "+----------+-------------------+-----------+-----------+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from etl.purchase_attribution import get_sessions, get_purchase_attribution\n",
    "\n",
    "sessions = get_sessions(click_stream)\n",
    "sessions.show()\n",
    "\n",
    "purchase_attribution = get_purchase_attribution(click_stream, sessions, purchases)\n",
    "purchase_attribution.show()"
   ]
  },
  {
   "source": [
    "### Task 1.2 - Implement it by using a custom Aggregator or UDAF."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting etl/purchase_attribution_udf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile etl/purchase_attribution_udf.py\n",
    "\n",
    "import json\n",
    "import operator\n",
    "from pyspark.sql import types as T\n",
    "import pyspark.sql.functions as psf \n",
    "\n",
    "\n",
    "def collect_purchases(click_stream):\n",
    "    click_stream = sorted(click_stream, key=operator.itemgetter(1, 0))\n",
    "\n",
    "    purchases = []\n",
    "    current_sesh = None, None, None\n",
    "\n",
    "    for meta in click_stream:\n",
    "        event_id, _, event_type, event_attr = meta\n",
    "\n",
    "        if event_attr:\n",
    "            event_attr = json.loads(event_attr)\n",
    "\n",
    "        if event_type == 'app_close':\n",
    "            current_sesh = None, None, None\n",
    "\n",
    "        elif event_type == 'app_open':\n",
    "            try:\n",
    "                current_sesh = [\n",
    "                    event_id,\n",
    "                    event_attr['campaign_id'],\n",
    "                    event_attr['channel_id']\n",
    "                ]\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        elif event_type == 'purchase':\n",
    "            try:\n",
    "                purchase_id = event_attr['purchase_id']\n",
    "\n",
    "                purchases.append([\n",
    "                    purchase_id,\n",
    "                    *current_sesh\n",
    "                ])\n",
    "            except KeyError:\n",
    "                pass\n",
    "    \n",
    "    return purchases\n",
    "\n",
    "collect_purchases_udf = psf.udf(collect_purchases, T.ArrayType(T.ArrayType(T.StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Appending to etl/purchase_attribution_udf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a etl/purchase_attribution_udf.py\n",
    "\n",
    "def get_purchase_attribution_udf(click_stream, purchases):\n",
    "    purchases_stream = click_stream.where(\n",
    "        psf.col('eventType').isin({'app_open', 'app_close', 'purchase'})\n",
    "    ).withColumn(\n",
    "        'purchase_meta',\n",
    "        psf.array(psf.col('eventId'), psf.col('eventTime'), psf.col('eventType'), psf.col('attributes'))\n",
    "    ).groupBy(\n",
    "        psf.col('userId')\n",
    "    ).agg(\n",
    "        collect_purchases_udf(psf.collect_list('purchase_meta')).alias('purchases')\n",
    "    ).withColumn(\n",
    "        'purchase',\n",
    "        psf.explode(psf.col('purchases'))\n",
    "    ).select(\n",
    "        psf.col('purchase')[0].alias('purchaseStreamId'),\n",
    "        psf.col('purchase')[1].alias('sessionId'),\n",
    "        psf.col('purchase')[2].alias('campaignId'),\n",
    "        psf.col('purchase')[3].alias('channelId')\n",
    "    )\n",
    "\n",
    "    purchase_attribution = purchases_stream.join(\n",
    "        purchases, purchases.purchaseId == purchases_stream.purchaseStreamId\n",
    "    ).select(\n",
    "        'purchaseId', 'purchaseTime', 'billingCost', 'isConfirmed', 'sessionId', 'campaignId', 'channelId'\n",
    "    ).orderBy(\n",
    "        'purchaseTime'\n",
    "    )\n",
    "\n",
    "    return purchase_attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-------------------+-----------+-----------+---------+----------+----------+\n|purchaseId|       purchaseTime|billingCost|isConfirmed|sessionId|campaignId| channelId|\n+----------+-------------------+-----------+-----------+---------+----------+----------+\n|        p1| 2019-01-01 0:01:05|      100.5|       TRUE|    u1_e1|      cmp1|Google Ads|\n|        p2| 2019-01-01 0:03:10|        200|       TRUE|    u2_e1|      cmp1|Yandex Ads|\n|        p3| 2019-01-01 1:12:15|        300|      FALSE|    u3_e5|      cmp1|Google Ads|\n|        p4| 2019-01-01 2:13:05|       50.2|       TRUE|   u3_e10|      cmp2|Yandex Ads|\n|        p5| 2019-01-01 2:15:05|         75|       TRUE|   u3_e10|      cmp2|Yandex Ads|\n|        p6|2019-01-02 13:03:00|         99|      FALSE|   u3_e19|      cmp2|Yandex Ads|\n+----------+-------------------+-----------+-----------+---------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from etl.purchase_attribution_udf import get_purchase_attribution_udf\n",
    "\n",
    "purchase_attribution_udf = get_purchase_attribution_udf(click_stream, purchases)\n",
    "purchase_attribution_udf.show()"
   ]
  },
  {
   "source": [
    "### Complete solution for the first task:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting task1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as psf \n",
    "\n",
    "from etl.purchase_attribution import get_sessions, get_purchase_attribution\n",
    "\n",
    "def main(click_stream_input, purchases_input, format_):\n",
    "    pyspark = SparkSession.builder.master(\"local[2]\").getOrCreate()\n",
    "\n",
    "    options = dict()\n",
    "    tsv_options = {'sep': r'\\t', 'header': True}\n",
    "    if format_ == 'tsv':\n",
    "        format_ = 'csv'\n",
    "        options = {**tsv_options}\n",
    "\n",
    "    click_stream = pyspark.read.format(format_).load(click_stream_input, **options)\n",
    "    purchases = pyspark.read.format(format_).load(purchases_input, **options)  \n",
    "\n",
    "    sessions = get_sessions(click_stream)\n",
    "    purchase_attribution = get_purchase_attribution(click_stream, sessions, purchases)\n",
    "\n",
    "    for df, timestamp_f, file_name in (\n",
    "            (purchase_attribution, 'purchaseTime', 'purchase_attribution'),\n",
    "            (sessions, 'sessionStart', 'sessions')\n",
    "        ):\n",
    "\n",
    "        df.withColumn(\n",
    "            'year', psf.year(timestamp_f)\n",
    "        ).withColumn(\n",
    "            'month', psf.month(timestamp_f)\n",
    "        ).withColumn(\n",
    "            'day', psf.dayofmonth(timestamp_f)\n",
    "        ).write.partitionBy(\n",
    "            'year', 'month', 'day'\n",
    "        ).mode(\"overwrite\").parquet(f'warehouse/{file_name}.parquet')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('click_stream_input', type=str)\n",
    "    parser.add_argument('purchases_input', type=str)\n",
    "    parser.add_argument('format', choices=('parquet', 'tsv'))\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args.click_stream_input, args.purchases_input, args.format)\n"
   ]
  },
  {
   "source": [
    "## Task 2 - Calculate Marketing Campaigns And Channels Statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Task 2.1 - Top Campaigns: What are the Top 10 marketing campaigns that bring the biggest revenue (based on billingCost of confirmed purchases)?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting etl/stats.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile etl/stats.py\n",
    "\n",
    "def get_top_campaigns(pyspark, purchase_attribution):\n",
    "\n",
    "    purchase_attribution.createOrReplaceTempView('purchase_attribution')\n",
    "\n",
    "    top_campaigns = pyspark.sql(\n",
    "        'SELECT campaignId, SUM(billingCost) as revenue '\n",
    "        'FROM purchase_attribution '\n",
    "        'WHERE isConfirmed = \"TRUE\" '\n",
    "        'GROUP BY campaignId '\n",
    "        'ORDER BY revenue DESC '\n",
    "        'LIMIT 10'\n",
    "    )\n",
    "\n",
    "    return top_campaigns"
   ]
  },
  {
   "source": [
    "### Task 2.2 - Channels engagement performance: What is the most popular (i.e. Top) channel that drives the highest amount of unique sessions (engagements) with the App in each campaign?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Appending to etl/stats.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a etl/stats.py\n",
    "\n",
    "def get_top_channels(pyspark, sessions):\n",
    "\n",
    "    sessions.createOrReplaceTempView('sessions')\n",
    "\n",
    "    top_channels = pyspark.sql(\n",
    "        'SELECT campaignId, channelId, performance '\n",
    "        'FROM ( '\n",
    "            'SELECT '\n",
    "                'ROW_NUMBER() OVER (PARTITION BY campaignId ORDER BY performance DESC) as rn, '\n",
    "                'campaignId, channelId, performance '\n",
    "            'FROM ( '\n",
    "                'SELECT '\n",
    "                    'campaignId, '\n",
    "                    'channelId, '\n",
    "                    'COUNT(DISTINCT(userId)) as performance ' \n",
    "                'FROM sessions '\n",
    "                'GROUP BY campaignId, channelId '\n",
    "                'ORDER BY performance DESC '\n",
    "            ') '\n",
    "        ') '\n",
    "        'WHERE rn=1 '\n",
    "        'ORDER BY campaignId, channelId'\n",
    "    )\n",
    "\n",
    "    return top_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-------+\n",
      "|campaignId|revenue|\n",
      "+----------+-------+\n",
      "|      cmp1|  300.5|\n",
      "|      cmp2|  125.2|\n",
      "+----------+-------+\n",
      "\n",
      "+----------+----------+-----------+\n",
      "|campaignId| channelId|performance|\n",
      "+----------+----------+-----------+\n",
      "|      cmp1|Google Ads|          2|\n",
      "|      cmp2|Yandex Ads|          2|\n",
      "+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from etl.stats import get_top_campaigns, get_top_channels\n",
    "\n",
    "top_campaigns = get_top_campaigns(pyspark, purchase_attribution)\n",
    "top_campaigns.show()\n",
    "\n",
    "top_channels = get_top_channels(pyspark, sessions)\n",
    "top_channels.show()"
   ]
  },
  {
   "source": [
    "### Complete solution for the second task:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting task2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task2.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from etl.stats import get_top_campaigns, get_top_channels\n",
    "\n",
    "def main(purchase_attribution_input, sessions_input, format_):\n",
    "    pyspark = SparkSession.builder.master(\"local[2]\").getOrCreate()\n",
    "\n",
    "    options = dict()\n",
    "    tsv_options = {'sep': r'\\t', 'header': True}\n",
    "    if format_ == 'tsv':\n",
    "        format_ = 'csv'\n",
    "        options = {**tsv_options}\n",
    "\n",
    "    purchase_attribution = pyspark.read.format(format_).load(purchase_attribution_input, **options)\n",
    "    sessions = pyspark.read.format(format_).load(sessions_input, **options)  \n",
    "\n",
    "    top_campaigns = get_top_campaigns(pyspark, purchase_attribution)\n",
    "    top_channels = get_top_channels(pyspark, sessions)\n",
    "\n",
    "    top_campaigns.show()\n",
    "    top_channels.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('purchase_attribution_input', type=str)\n",
    "    parser.add_argument('sessions_input', type=str)\n",
    "    parser.add_argument('format', choices=('parquet', 'tsv'))\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args.purchase_attribution_input, args.sessions_input, args.format)\n"
   ]
  },
  {
   "source": [
    "## Task 3 - Organize data warehouse and calculate metrics for time period.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Task 3.1 - Convert input dataset to parquet. Think about partitioning. Compare performance on top CSV input and parquet input. Save output for Task #1 as parquet as well."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyspark.sql.functions as psf \n",
    "\n",
    "for format_ in ['parquet', 'tsv']:\n",
    "    for df, timestamp_f, file_name in (\n",
    "            (click_stream, 'eventTime', 'clickstream_sample'),\n",
    "            (purchases, 'purchaseTime', 'purchases_sample'),\n",
    "            (purchase_attribution, 'purchaseTime', 'purchase_attribution'),\n",
    "            (sessions, 'sessionStart', 'sessions')\n",
    "        ):\n",
    "\n",
    "        real_format = format_\n",
    "        options = dict()\n",
    "        tsv_options = {'sep': r'\\t', 'header': True}\n",
    "        if format_ == 'tsv':\n",
    "            real_format = 'csv'\n",
    "            options = {**tsv_options}\n",
    "\n",
    "\n",
    "        df = df.withColumn(\n",
    "            'year', psf.year(timestamp_f)\n",
    "        ).withColumn(\n",
    "            'month', psf.month(timestamp_f)\n",
    "        ).withColumn(\n",
    "            'day', psf.dayofmonth(timestamp_f)\n",
    "        )\n",
    "        writer = df.write\n",
    "        \n",
    "        if format_ == 'parquet':\n",
    "            df = writer.partitionBy('year', 'month', 'day')\n",
    "            \n",
    "        writer.mode(\"overwrite\").format(real_format).save(f'warehouse/{file_name}.{format_}', **options)\n"
   ]
  },
  {
   "source": [
    "### CSV - Task 1 integrational test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "20/12/28 00:10:30 WARN Utils: Your hostname, C8119 resolves to a loopback address: 127.0.0.1; using 192.168.31.239 instead (on interface en0)\n20/12/28 00:10:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nWARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/akepko/Documents/courses/aws_big_data/grid_capstone/.venv/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n20/12/28 00:10:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n20/12/28 00:10:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n                                                                                \nreal\t0m13.370s\nuser\t0m0.175s\nsys\t0m0.050s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "time ./.venv/bin/python3 task1.py warehouse/clickstream_sample.tsv warehouse/purchases_sample.tsv tsv\n"
   ]
  },
  {
   "source": [
    "### CSV - Task 1 unit test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "824 ms ± 52.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "click_stream = pyspark.read.csv('warehouse/clickstream_sample.tsv', sep=r'\\t', header=True)\n",
    "purchases = pyspark.read.csv('warehouse/purchases_sample.tsv', sep=r'\\t', header=True)\n",
    "\n",
    "sessions = get_sessions(click_stream)\n",
    "purchase_attribution = get_purchase_attribution(click_stream, sessions, purchases)\n",
    "purchase_attribution.count()"
   ]
  },
  {
   "source": [
    "### Parquet - Task 1 integrational test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "20/12/28 00:10:51 WARN Utils: Your hostname, C8119 resolves to a loopback address: 127.0.0.1; using 192.168.31.239 instead (on interface en0)\n20/12/28 00:10:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nWARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/akepko/Documents/courses/aws_big_data/grid_capstone/.venv/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n20/12/28 00:10:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n20/12/28 00:10:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n                                                                                \nreal\t0m12.943s\nuser\t0m0.174s\nsys\t0m0.053s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "time ./.venv/bin/python3 task1.py warehouse/clickstream_sample.parquet warehouse/purchases_sample.parquet parquet\n"
   ]
  },
  {
   "source": [
    "### Parquet - Task 1 unit test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "777 ms ± 61.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "click_stream_parquet = pyspark.read.parquet('warehouse/clickstream_sample.parquet')\n",
    "purchases_parquet = pyspark.read.parquet('warehouse/purchases_sample.parquet')\n",
    "\n",
    "sessions_parquet = get_sessions(click_stream_parquet)\n",
    "purchase_attribution = get_purchase_attribution(click_stream_parquet, sessions_parquet, purchases_parquet)\n",
    "purchase_attribution.count()"
   ]
  },
  {
   "source": [
    "### Task 3.2 - Calculate metrics from Task #2 for different time periods:\n",
    "\n",
    "For September 2020\n",
    "\n",
    "For 2020-11-11\n",
    "\n",
    "Compare performance on top csv input and partitioned parquet input. Print and analyze query plans (logical and phisical) for both inputs. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_purchase_attribution_tsv():\n",
    "    purchase_attribution = pyspark.read.csv('warehouse/purchase_attribution.tsv', sep=r'\\t', header=True)\n",
    "    sessions = pyspark.read.csv('warehouse/sessions.tsv', sep=r'\\t', header=True)\n",
    "\n",
    "    return purchase_attribution, sessions\n",
    "\n",
    "def get_purchase_attribution_parquet():\n",
    "    purchase_attribution_parquet = pyspark.read.parquet('warehouse/purchase_attribution.parquet')\n",
    "    sessions_parquet = pyspark.read.parquet('warehouse/sessions.parquet')\n",
    "\n",
    "    return purchase_attribution_parquet, sessions_parquet\n",
    "\n",
    "def get_2020_9(purchase_attribution, sessions):\n",
    "    purchase_attribution = purchase_attribution.where(psf.col('year') == 2020).where(psf.col('month') == 9)\n",
    "    sessions = sessions.where(psf.col('year') == 2020).where(psf.col('month') == 9)\n",
    "\n",
    "    return purchase_attribution, sessions\n",
    "\n",
    "def get_2020_11_11(purchase_attribution, sessions):\n",
    "    purchase_attribution = purchase_attribution.where(\n",
    "        psf.col('year') == 2020\n",
    "    ).where(\n",
    "        psf.col('month') == 11\n",
    "    ).where(\n",
    "        psf.col('day') == 11\n",
    "    )\n",
    "    sessions = sessions.where(\n",
    "        psf.col('year') == 2020\n",
    "    ).where(\n",
    "        psf.col('month') == 11\n",
    "    ).where(\n",
    "        psf.col('day') == 11\n",
    "    )\n",
    "\n",
    "    return purchase_attribution, sessions\n"
   ]
  },
  {
   "source": [
    "### CSV 2020-09"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.37 s ± 78.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "purchase_attribution, sessions = get_2020_9(*get_purchase_attribution_tsv())\n",
    "\n",
    "top_campaigns = get_top_campaigns(pyspark, purchase_attribution)\n",
    "top_campaigns.count()\n",
    "top_channels = get_top_channels(pyspark, sessions)\n",
    "top_channels.count()"
   ]
  },
  {
   "source": [
    "### CSV 2020-11-11"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.19 s ± 32.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "purchase_attribution, sessions = get_2020_11_11(*get_purchase_attribution_tsv())\n",
    "\n",
    "top_campaigns = get_top_campaigns(pyspark, purchase_attribution)\n",
    "top_campaigns.count()\n",
    "top_channels = get_top_channels(pyspark, sessions)\n",
    "top_channels.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "purchase_attribution, sessions = get_2020_11_11(*get_purchase_attribution_tsv())\n",
    "\n",
    "top_campaigns = get_top_campaigns(pyspark, purchase_attribution)\n",
    "top_campaigns.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plans/csv_top_campaigns_2020_11_11.md', 'w') as f:\n",
    "     f.write(cap.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "top_channels = get_top_channels(pyspark, sessions)\n",
    "top_channels.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plans/csv_top_channels_2020_11_11.md', 'w') as f:\n",
    "     f.write(cap.stdout)"
   ]
  },
  {
   "source": [
    "### Parquet 2020-09"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.15 s ± 196 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "purchase_attribution, sessions = get_2020_9(*get_purchase_attribution_parquet())\n",
    "\n",
    "top_campaigns = get_top_campaigns(pyspark, purchase_attribution)\n",
    "top_campaigns.count()\n",
    "top_channels = get_top_channels(pyspark, sessions)\n",
    "top_channels.count()"
   ]
  },
  {
   "source": [
    "### Parquet 2020-11-11"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.98 s ± 130 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "purchase_attribution, sessions = get_2020_11_11(*get_purchase_attribution_parquet())\n",
    "\n",
    "top_campaigns = get_top_campaigns(pyspark, purchase_attribution)\n",
    "top_campaigns.count()\n",
    "top_channels = get_top_channels(pyspark, sessions)\n",
    "top_channels.count()"
   ]
  },
  {
   "source": [
    "%%capture cap --no-stderr\n",
    "purchase_attribution, sessions = get_2020_11_11(*get_purchase_attribution_tsv())\n",
    "\n",
    "top_campaigns = get_top_campaigns(pyspark, purchase_attribution)\n",
    "top_campaigns.explain(extended=True)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plans/parquet_top_campaigns_2020_11_11.md', 'w') as f:\n",
    "     f.write(cap.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "top_channels = get_top_channels(pyspark, sessions)\n",
    "top_channels.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plans/parquet_top_channels_2020_11_11.md', 'w') as f:\n",
    "     f.write(cap.stdout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "d94517a5ee72d35ce813725207242343d3141e6bb8d91223726cc4c2a9c8aa3d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}